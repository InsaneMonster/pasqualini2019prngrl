experiment:
Data:

Vanilla Policy Gradient with GAE buffer
Six dense layer with 4096 neurons each using xavier initialization
Learning rate policy: 0.00003
Learning rate advantage: 0.0001
Discount factor: 0.60
Value steps per update: 80
Lambda parameter: 0.95
Updates per training volley: 20
Success threshold: 0.25 scaled reward on average over last 20 steps among all the validation volleys episodes
Max allowed steps for episode: 150
Acceptance value: none


Results:

Success  Average total reward  Max total reward  Average scaled reward  Max scaled reward  Trained episodes
    YES                 8.052             10.55                  0.054               0.07            100000



Maximum average total reward over all 1000 test episodes: 8.052 at experiment number 0
Maximum max total score over 100 test episodes: 10.550 at experiment number 0
Maximum average scaled reward over all 1000 test episodes: 0.054 at experiment number 0
Maximum max scaled score over 100 test episodes: 0.070 at experiment number 0
Minimum training episodes for validation: 100000 at experiment number 0
Average of average total reward over all experiments 8.052
Average of max total reward over all experiments: 10.550
Average of average scaled reward over all experiments 0.054
Average of max scaled reward over all experiments: 0.070
Average of training episodes over all experiments: 100000.000
Success percentage over all experiments: 100.00
Standard deviation of average total reward over all experiments: 0.000, which is a 0.00% of the average
Standard deviation of max total reward over all experiments 0.000, which is a 0.00% of the average
Standard deviation of average scaled reward over all experiments: 0.000, which is a 0.00% of the average
Standard deviation of max scaled reward over all experiments 0.000, which is a 0.00% of the average
Standard deviation of training episodes over all experiments: 0.000, which is a 0.00% of the average
